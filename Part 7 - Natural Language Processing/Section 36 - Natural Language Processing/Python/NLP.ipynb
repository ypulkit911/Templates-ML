{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "from scipy.spatial import distance\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (1.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (4.47.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (8.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (0.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyterlab-kite>=2.0.2 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: jupyterlab<4.0.0a0,>=3.0.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab-kite>=2.0.2) (3.0.16)\n",
      "Requirement already satisfied: jupyter-kite>=2.0.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab-kite>=2.0.2) (2.0.2)\n",
      "Requirement already satisfied: nbclassic~=0.2 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.2.8)\n",
      "Requirement already satisfied: packaging in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (20.4)\n",
      "Requirement already satisfied: tornado>=6.1.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (6.1)\n",
      "Requirement already satisfied: jupyter-server~=1.4 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (1.7.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (4.6.3)\n",
      "Requirement already satisfied: jinja2>=2.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (2.11.2)\n",
      "Requirement already satisfied: ipython in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (7.16.1)\n",
      "Requirement already satisfied: jupyterlab-server~=2.3 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (2.5.2)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-kite>=2.0.0->jupyterlab-kite>=2.0.2) (0.3)\n",
      "Requirement already satisfied: notebook<7 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from nbclassic~=0.2->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (6.0.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from packaging->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from packaging->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (1.15.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (19.0.1)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (1.5.0)\n",
      "Requirement already satisfied: nbformat in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (5.0.7)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (20.1.0)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (4.3.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (5.6.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (6.1.6)\n",
      "Requirement already satisfied: anyio<4,>=3.0.1; python_version >= \"3.7\" in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (3.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.8.3)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.8.0)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (1.0.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.2.0)\n",
      "Requirement already satisfied: pywin32>=1.0; sys_platform == \"win32\" in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-core->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (227)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jinja2>=2.1->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (1.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from ipython->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.7.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from ipython->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (4.4.2)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from ipython->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.4.3)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from ipython->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.17.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from ipython->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (3.0.5)\n",
      "Requirement already satisfied: backcall in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from ipython->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from ipython->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (49.2.0.post20200714)\n",
      "Requirement already satisfied: pygments in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from ipython->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (2.6.1)\n",
      "Requirement already satisfied: babel in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab-server~=2.3->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (2.8.0)\n",
      "Requirement already satisfied: json5 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab-server~=2.3->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.9.5)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab-server~=2.3->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (3.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyterlab-server~=2.3->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (2.24.0)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from notebook<7->nbclassic~=0.2->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (5.3.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from argon2-cffi->jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (1.14.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from nbconvert->jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.8.4)\n",
      "Requirement already satisfied: bleach in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from nbconvert->jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (3.1.5)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from nbconvert->jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.6.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from nbconvert->jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (1.4.2)\n",
      "Requirement already satisfied: testpath in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from nbconvert->jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.1->jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (2.8.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from anyio<4,>=3.0.1; python_version >= \"3.7\"->jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from anyio<4,>=3.0.1; python_version >= \"3.7\"->jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (2.10)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.2.5)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from babel->jupyterlab-server~=2.3->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (2020.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.3->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.16.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.3->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (19.3.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from requests->jupyterlab-server~=2.3->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from requests->jupyterlab-server~=2.3->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from requests->jupyterlab-server~=2.3->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (1.25.9)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (2.20)\n",
      "Requirement already satisfied: webencodings in c:\\users\\ypulk\\anaconda3\\lib\\site-packages (from bleach->nbconvert->jupyter-server~=1.4->jupyterlab<4.0.0a0,>=3.0.0->jupyterlab-kite>=2.0.2) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"jupyterlab-kite>=2.0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ypulk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ypulk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ypulk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ypulk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# order : re, tokenize, stemmer, lemmatize, remove stopwords, pos_tag, chunking(regexpParser)\n",
    "#2 (methods) : CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  #tokenizing of words\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re #to remove spaces etc\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer  #portStemmer is to remove the suffix and prefix, Lemmatizer shreads the words to root\n",
    "from nltk.corpus import stopwords #useless words \n",
    "nltk.download('stopwords')\n",
    "from nltk import pos_tag  #check if its a noun or verb or whatever, NO NEED TO ITERATE over Lemmatized words\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import RegexpParser #check out the use space\n",
    "#from nltk.np_chunk_counter import np_chunk_counter\n",
    "\n",
    "#from preprocessing import preprocess_text #simply \n",
    "\n",
    "\n",
    "\n",
    "#2 document:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "\n",
    "#3\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the one function to rule them all\n",
    "random_txt = \"We present a comprehensive introduction to text preprocessing, covering the different techniques including stemming, lemmatization, noise removal, normalization, with examples and explanations into when you should use each of them.\"\n",
    "\n",
    "def thefunction(x):\n",
    "    inputtext = x\n",
    "    lower_case = inputtext.lower()\n",
    "    \n",
    "    html_pattern = r'<.*?>'\n",
    "    \n",
    "    remove_html_tags = re.sub(pattern=html_pattern, repl=' ', string=inputtext)\n",
    "    \n",
    "    stupid_things_removed = re.sub(r'[\\.\\?\\!\\,\\:\\;\\\"]', \"\", lower_case)\n",
    "    \n",
    "    token_list = []\n",
    "    q = word_tokenize(stupid_things_removed)\n",
    "    for i in q:\n",
    "        token_list.append(i)\n",
    "        \n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(j) for j in token_list ]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(k) for k in stemmed_tokens]\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    filtered_tokens= []\n",
    "    for l in lemmatized_tokens:\n",
    "        if l not in stop_words:\n",
    "            filtered_tokens.append(l)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(lower_case)\n",
    "    \n",
    "    print(remove_html_tags)\n",
    "    \n",
    "    print(stupid_things_removed)\n",
    "    \n",
    "    print(token_list)\n",
    "    \n",
    "    print(stemmed_tokens)\n",
    "    \n",
    "    print(lemmatized_tokens)\n",
    "    \n",
    "    print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we present a comprehensive introduction to text preprocessing, covering the different techniques including stemming, lemmatization, noise removal, normalization, with examples and explanations into when you should use each of them.\n",
      "We present a comprehensive introduction to text preprocessing, covering the different techniques including stemming, lemmatization, noise removal, normalization, with examples and explanations into when you should use each of them.\n",
      "we present a comprehensive introduction to text preprocessing covering the different techniques including stemming lemmatization noise removal normalization with examples and explanations into when you should use each of them\n",
      "['we', 'present', 'a', 'comprehensive', 'introduction', 'to', 'text', 'preprocessing', 'covering', 'the', 'different', 'techniques', 'including', 'stemming', 'lemmatization', 'noise', 'removal', 'normalization', 'with', 'examples', 'and', 'explanations', 'into', 'when', 'you', 'should', 'use', 'each', 'of', 'them']\n",
      "['we', 'present', 'a', 'comprehens', 'introduct', 'to', 'text', 'preprocess', 'cover', 'the', 'differ', 'techniqu', 'includ', 'stem', 'lemmat', 'nois', 'remov', 'normal', 'with', 'exampl', 'and', 'explan', 'into', 'when', 'you', 'should', 'use', 'each', 'of', 'them']\n",
      "['we', 'present', 'a', 'comprehens', 'introduct', 'to', 'text', 'preprocess', 'cover', 'the', 'differ', 'techniqu', 'includ', 'stem', 'lemmat', 'nois', 'remov', 'normal', 'with', 'exampl', 'and', 'explan', 'into', 'when', 'you', 'should', 'use', 'each', 'of', 'them']\n",
      "['present', 'comprehens', 'introduct', 'text', 'preprocess', 'cover', 'differ', 'techniqu', 'includ', 'stem', 'lemmat', 'nois', 'remov', 'normal', 'exampl', 'explan', 'use']\n"
     ]
    }
   ],
   "source": [
    "thefunction(random_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The club was estimated to be worth €3.8 billion ($4.2 billion) in 2019, and it was the second highest-earning football club in the world, with an annual revenue of €757.3 million in 2019.[7][8] The club is one of the most widely supported teams in the world.[9] Real Madrid is one of three founding members of La Liga that have never been relegated from the top division since its inception in 1929, along with Athletic Bilbao and Barcelona. The club holds many long-standing rivalries, most notably El Clásico with Barcelona and El Derbi Madrileño with Atlético Madrid.\n"
     ]
    }
   ],
   "source": [
    "a = \"The club was estimated to be worth €3.8 billion ($4.2 billion) in 2019, and it was the second highest-earning football club in the world, with an annual revenue of €757.3 million in 2019.[7][8] The club is one of the most widely supported teams in the world.[9] Real Madrid is one of three founding members of La Liga that have never been relegated from the top division since its inception in 1929, along with Athletic Bilbao and Barcelona. The club holds many long-standing rivalries, most notably El Clásico with Barcelona and El Derbi Madrileño with Atlético Madrid.\"\n",
    "print(a)\n",
    "k = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The club was estimated to be worth €38 billion ($42 billion) in 2019 and it was the second highest-earning football club in the world with an annual revenue of €7573 million in 2019[7][8] The club is one of the most widely supported teams in the world[9] Real Madrid is one of three founding members of La Liga that have never been relegated from the top division since its inception in 1929 along with Athletic Bilbao and Barcelona The club holds many long-standing rivalries most notably El Clásico with Barcelona and El Derbi Madrileño with Atlético Madrid\n"
     ]
    }
   ],
   "source": [
    "result = re.sub(r'[\\.\\?\\!\\,\\:\\;\\\"]', \"\", a)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'club', 'was', 'estimated', 'to', 'be', 'worth', '€38', 'billion', '(', '$', '42', 'billion', ')', 'in', '2019', 'and', 'it', 'was', 'the', 'second', 'highest-earning', 'football', 'club', 'in', 'the', 'world', 'with', 'an', 'annual', 'revenue', 'of', '€7573', 'million', 'in', '2019', '[', '7', ']', '[', '8', ']', 'The', 'club', 'is', 'one', 'of', 'the', 'most', 'widely', 'supported', 'teams', 'in', 'the', 'world', '[', '9', ']', 'Real', 'Madrid', 'is', 'one', 'of', 'three', 'founding', 'members', 'of', 'La', 'Liga', 'that', 'have', 'never', 'been', 'relegated', 'from', 'the', 'top', 'division', 'since', 'its', 'inception', 'in', '1929', 'along', 'with', 'Athletic', 'Bilbao', 'and', 'Barcelona', 'The', 'club', 'holds', 'many', 'long-standing', 'rivalries', 'most', 'notably', 'El', 'Clásico', 'with', 'Barcelona', 'and', 'El', 'Derbi', 'Madrileño', 'with', 'Atlético', 'Madrid']\n"
     ]
    }
   ],
   "source": [
    "x = word_tokenize(result)\n",
    "for i in x:\n",
    "    k.append(i)\n",
    "\n",
    "print(k)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'club', 'was', 'estimated', 'to', 'be', 'worth', '€38', 'billion', '(', '$', '42', 'billion', ')', 'in', '2019', 'and', 'it', 'was', 'the', 'second', 'highest-earning', 'football', 'club', 'in', 'the', 'world', 'with', 'an', 'annual', 'revenue', 'of', '€7573', 'million', 'in', '2019', '[', '7', ']', '[', '8', ']', 'The', 'club', 'is', 'one', 'of', 'the', 'most', 'widely', 'supported', 'teams', 'in', 'the', 'world', '[', '9', ']', 'Real', 'Madrid', 'is', 'one', 'of', 'three', 'founding', 'members', 'of', 'La', 'Liga', 'that', 'have', 'never', 'been', 'relegated', 'from', 'the', 'top', 'division', 'since', 'its', 'inception', 'in', '1929', 'along', 'with', 'Athletic', 'Bilbao', 'and', 'Barcelona', 'The', 'club', 'holds', 'many', 'long-standing', 'rivalries', 'most', 'notably', 'El', 'Clásico', 'with', 'Barcelona', 'and', 'El', 'Derbi', 'Madrileño', 'with', 'Atlético', 'Madrid']\n"
     ]
    }
   ],
   "source": [
    "tokenized = word_tokenize(result)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'club', 'wa', 'estim', 'to', 'be', 'worth', '€38', 'billion', '(', '$', '42', 'billion', ')', 'in', '2019', 'and', 'it', 'wa', 'the', 'second', 'highest-earn', 'footbal', 'club', 'in', 'the', 'world', 'with', 'an', 'annual', 'revenu', 'of', '€7573', 'million', 'in', '2019', '[', '7', ']', '[', '8', ']', 'the', 'club', 'is', 'one', 'of', 'the', 'most', 'wide', 'support', 'team', 'in', 'the', 'world', '[', '9', ']', 'real', 'madrid', 'is', 'one', 'of', 'three', 'found', 'member', 'of', 'La', 'liga', 'that', 'have', 'never', 'been', 'releg', 'from', 'the', 'top', 'divis', 'sinc', 'it', 'incept', 'in', '1929', 'along', 'with', 'athlet', 'bilbao', 'and', 'barcelona', 'the', 'club', 'hold', 'mani', 'long-stand', 'rivalri', 'most', 'notabl', 'El', 'clásico', 'with', 'barcelona', 'and', 'El', 'derbi', 'madrileño', 'with', 'atlético', 'madrid']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(i) for i in tokenized ]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'club', 'wa', 'estim', 'to', 'be', 'worth', '€38', 'billion', '(', '$', '42', 'billion', ')', 'in', '2019', 'and', 'it', 'wa', 'the', 'second', 'highest-earn', 'footbal', 'club', 'in', 'the', 'world', 'with', 'an', 'annual', 'revenu', 'of', '€7573', 'million', 'in', '2019', '[', '7', ']', '[', '8', ']', 'the', 'club', 'is', 'one', 'of', 'the', 'most', 'wide', 'support', 'team', 'in', 'the', 'world', '[', '9', ']', 'real', 'madrid', 'is', 'one', 'of', 'three', 'found', 'member', 'of', 'La', 'liga', 'that', 'have', 'never', 'been', 'releg', 'from', 'the', 'top', 'divis', 'sinc', 'it', 'incept', 'in', '1929', 'along', 'with', 'athlet', 'bilbao', 'and', 'barcelona', 'the', 'club', 'hold', 'mani', 'long-stand', 'rivalri', 'most', 'notabl', 'El', 'clásico', 'with', 'barcelona', 'and', 'El', 'derbi', 'madrileño', 'with', 'atlético', 'madrid']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(i) for i in stemmed]\n",
    "print(lemmatized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"she's\", 'on', 'ours', 'has', 'of', 'itself', 'she', 'my', 'during', 'which', 've', 'didn', 'they', \"it's\", 'same', 'shouldn', 'myself', 'doesn', 'these', 'ma', \"mustn't\", 'other', 'about', 'needn', 'our', 'now', \"aren't\", 's', 'isn', 'all', 'in', 'he', 'being', 'you', 'does', \"haven't\", 'wasn', 'that', 'weren', 'can', 'and', 'y', 'or', 'did', 'an', 'mustn', 'out', 'are', 'his', 'don', 'after', 'few', 'where', 'not', 'i', 'most', 'who', 'theirs', 'its', 'had', 'while', 'no', 'yours', \"don't\", 'is', 'shan', 'here', 'than', 'very', 'any', 'with', 'will', 'do', 'above', 'below', 'some', 'll', 'hers', 'too', 'her', 'against', 'we', 'it', 'yourselves', \"shouldn't\", 'until', 'am', 'those', 'if', 'when', 'the', 'a', 'both', 'himself', 'because', 'hadn', 'just', 'by', 'aren', 'whom', 'then', 'so', 'won', 'further', 'be', 'o', \"shan't\", \"won't\", \"should've\", \"couldn't\", 'such', 'there', 'themselves', 'ain', 'at', 'm', \"you've\", 'having', \"hadn't\", 'me', 'them', 'down', \"doesn't\", 'haven', 'each', 'for', \"weren't\", 't', 'him', 'from', \"wasn't\", 'yourself', 'nor', 'under', 'been', 'only', 'more', 'were', 'should', 'but', 'once', 'ourselves', 'through', 'how', 'have', 'as', 'was', 're', 'why', \"you'll\", 'wouldn', 'their', \"that'll\", 'over', \"wouldn't\", \"needn't\", \"mightn't\", 'herself', \"isn't\", \"hasn't\", 'between', 'mightn', 'this', 'off', 'd', \"you're\", 'doing', 'up', 'own', \"you'd\", 'your', 'before', 'again', \"didn't\", 'to', 'couldn', 'into', 'hasn', 'what'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n",
    "filtered = []\n",
    "o= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['club', 'wa', 'estim', 'worth', '€38', 'billion', '(', '$', '42', 'billion', ')', '2019', 'wa', 'second', 'highest-earn', 'footbal', 'club', 'world', 'annual', 'revenu', '€7573', 'million', '2019', '[', '7', ']', '[', '8', ']', 'club', 'one', 'wide', 'support', 'team', 'world', '[', '9', ']', 'real', 'madrid', 'one', 'three', 'found', 'member', 'La', 'liga', 'never', 'releg', 'top', 'divis', 'sinc', 'incept', '1929', 'along', 'athlet', 'bilbao', 'barcelona', 'club', 'hold', 'mani', 'long-stand', 'rivalri', 'notabl', 'El', 'clásico', 'barcelona', 'El', 'derbi', 'madrileño', 'atlético', 'madrid']\n"
     ]
    }
   ],
   "source": [
    "for i in lemmatized:\n",
    "    if i not in stop_words:\n",
    "        filtered.append(i)\n",
    "\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['club', 'wa', 'estim', 'worth', '€38', 'billion', '(', '$', '42', 'billion', ')', '2019', 'wa', 'second', 'highest-earn', 'footbal', 'club', 'world', 'annual', 'revenu', '€7573', 'million', '2019', '[', '7', ']', '[', '8', ']', 'club', 'one', 'wide', 'support', 'team', 'world', '[', '9', ']', 'real', 'madrid', 'one', 'three', 'found', 'member', 'La', 'liga', 'never', 'releg', 'top', 'divis', 'sinc', 'incept', '1929', 'along', 'athlet', 'bilbao', 'barcelona', 'club', 'hold', 'mani', 'long-stand', 'rivalri', 'notabl', 'El', 'clásico', 'barcelona', 'El', 'derbi', 'madrileño', 'atlético', 'madrid']\n"
     ]
    }
   ],
   "source": [
    "o = [w for w in filtered if w not in stop_words]\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('club', 'NN'), ('wa', 'NN'), ('estim', 'NN'), ('worth', 'JJ'), ('€38', 'NNP'), ('billion', 'CD'), ('(', '('), ('$', '$'), ('42', 'CD'), ('billion', 'CD'), (')', ')'), ('2019', 'CD'), ('wa', 'JJ'), ('second', 'JJ'), ('highest-earn', 'NN'), ('footbal', 'NN'), ('club', 'NN'), ('world', 'NN'), ('annual', 'JJ'), ('revenu', 'NN'), ('€7573', 'CD'), ('million', 'CD'), ('2019', 'CD'), ('[', 'NN'), ('7', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('8', 'CD'), (']', 'NN'), ('club', 'NN'), ('one', 'CD'), ('wide', 'JJ'), ('support', 'NN'), ('team', 'NN'), ('world', 'NN'), ('[', 'VBD'), ('9', 'CD'), (']', 'JJ'), ('real', 'JJ'), ('madrid', 'NN'), ('one', 'CD'), ('three', 'CD'), ('found', 'IN'), ('member', 'NN'), ('La', 'NNP'), ('liga', 'VBZ'), ('never', 'RB'), ('releg', 'VBN'), ('top', 'JJ'), ('divis', 'NN'), ('sinc', 'NN'), ('incept', 'IN'), ('1929', 'CD'), ('along', 'IN'), ('athlet', 'NN'), ('bilbao', 'NN'), ('barcelona', 'NN'), ('club', 'NN'), ('hold', 'VBP'), ('mani', 'JJ'), ('long-stand', 'JJ'), ('rivalri', 'NN'), ('notabl', 'NN'), ('El', 'NNP'), ('clásico', 'NN'), ('barcelona', 'NN'), ('El', 'NNP'), ('derbi', 'NN'), ('madrileño', 'NN'), ('atlético', 'NN'), ('madrid', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagged = pos_tag(o)\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  club/NN\n",
      "  wa/NN\n",
      "  estim/NN\n",
      "  worth/JJ\n",
      "  €38/NNP\n",
      "  billion/CD\n",
      "  (/(\n",
      "  $/$\n",
      "  42/CD\n",
      "  billion/CD\n",
      "  )/)\n",
      "  2019/CD\n",
      "  wa/JJ\n",
      "  (AN second/JJ highest-earn/NN)\n",
      "  footbal/NN\n",
      "  club/NN\n",
      "  world/NN\n",
      "  (AN annual/JJ revenu/NN)\n",
      "  €7573/CD\n",
      "  million/CD\n",
      "  2019/CD\n",
      "  [/NN\n",
      "  7/CD\n",
      "  ]/NNP\n",
      "  [/VBD\n",
      "  8/CD\n",
      "  ]/NN\n",
      "  club/NN\n",
      "  one/CD\n",
      "  (AN wide/JJ support/NN)\n",
      "  team/NN\n",
      "  world/NN\n",
      "  [/VBD\n",
      "  9/CD\n",
      "  ]/JJ\n",
      "  (AN real/JJ madrid/NN)\n",
      "  one/CD\n",
      "  three/CD\n",
      "  found/IN\n",
      "  member/NN\n",
      "  La/NNP\n",
      "  liga/VBZ\n",
      "  never/RB\n",
      "  releg/VBN\n",
      "  (AN top/JJ divis/NN)\n",
      "  sinc/NN\n",
      "  incept/IN\n",
      "  1929/CD\n",
      "  along/IN\n",
      "  athlet/NN\n",
      "  bilbao/NN\n",
      "  barcelona/NN\n",
      "  club/NN\n",
      "  hold/VBP\n",
      "  mani/JJ\n",
      "  (AN long-stand/JJ rivalri/NN)\n",
      "  notabl/NN\n",
      "  El/NNP\n",
      "  clásico/NN\n",
      "  barcelona/NN\n",
      "  El/NNP\n",
      "  derbi/NN\n",
      "  madrileño/NN\n",
      "  atlético/NN\n",
      "  madrid/NN)\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chunk_grammar = \"AN : {<JJ><NN>}\" #AN is the name, JJ and NN are adjectives and noun, weird syntax\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "chunked = chunk_parser.parse(pos_tagged)\n",
    "print(chunked) \n",
    "print(len(chunked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12)\t1\n",
      "  (1, 46)\t1\n",
      "  (2, 18)\t1\n",
      "  (3, 49)\t1\n",
      "  (4, 2)\t1\n",
      "  (5, 11)\t1\n",
      "  (8, 3)\t1\n",
      "  (9, 11)\t1\n",
      "  (11, 1)\t1\n",
      "  (12, 46)\t1\n",
      "  (13, 39)\t1\n",
      "  (14, 21)\t1\n",
      "  (14, 16)\t1\n",
      "  (15, 19)\t1\n",
      "  (16, 12)\t1\n",
      "  (17, 48)\t1\n",
      "  (18, 6)\t1\n",
      "  (19, 37)\t1\n",
      "  (20, 4)\t1\n",
      "  (21, 31)\t1\n",
      "  (22, 1)\t1\n",
      "  (29, 12)\t1\n",
      "  (30, 34)\t1\n",
      "  (31, 47)\t1\n",
      "  (32, 42)\t1\n",
      "  :\t:\n",
      "  (47, 36)\t1\n",
      "  (48, 45)\t1\n",
      "  (49, 15)\t1\n",
      "  (50, 40)\t1\n",
      "  (51, 23)\t1\n",
      "  (52, 0)\t1\n",
      "  (53, 5)\t1\n",
      "  (54, 7)\t1\n",
      "  (55, 10)\t1\n",
      "  (56, 9)\t1\n",
      "  (57, 12)\t1\n",
      "  (58, 22)\t1\n",
      "  (59, 29)\t1\n",
      "  (60, 26)\t1\n",
      "  (60, 41)\t1\n",
      "  (61, 38)\t1\n",
      "  (62, 33)\t1\n",
      "  (63, 17)\t1\n",
      "  (64, 13)\t1\n",
      "  (65, 9)\t1\n",
      "  (66, 17)\t1\n",
      "  (67, 14)\t1\n",
      "  (68, 28)\t1\n",
      "  (69, 8)\t1\n",
      "  (70, 27)\t1\n"
     ]
    }
   ],
   "source": [
    "training_vectors = bow_vectorizer.fit_transform(o)\n",
    "print(training_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'out': 8, 'of': 7, 'all': 0, 'the': 13, 'countries': 3, 'world': 14, 'some': 12, 'are': 1, 'poor': 10, 'rich': 11, 'but': 2, 'no': 6, 'country': 4, 'is': 5, 'perfect': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ypulk\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass input=['Out of all the countries of the world, some countries are poor, some countries are rich, but no country is perfect.'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "test = [\"Out of all the countries of the world, some countries are poor, some countries are rich, but no country is perfect.\"]\n",
    "doc = CountVectorizer(test)\n",
    "cv_vector = doc.fit_transform(test)\n",
    "print(doc.vocabulary_)\n",
    "# https://www.studytonight.com/post/scikitlearn-countvectorizer-in-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_dictionary(document_tokens):         #first dictionary is created\n",
    "  features_dictionary = {}       \n",
    "  index = 0\n",
    "  for token in document_tokens:                         # each word in document is looped through, if not already in dict\n",
    "    if token not in features_dictionary:                # it is added to the dict with a new incremental index\n",
    "      features_dictionary[token] = index\n",
    "      index += 1\n",
    "  return features_dictionary\n",
    "\n",
    "def tokens_to_bow_vector(document_tokens, features_dictionary):       #vector is created, vector is essentially the number of times a word appears in the doc\n",
    "  bow_vector = [0] * len(features_dictionary)                         #length of the vector is length of the dict \n",
    "  for token in document_tokens:\n",
    "    if token in features_dictionary:                                 # if the word of the doc is in the dic, the count of that \n",
    "      feature_index = features_dictionary[token]                    # word increases in the vector\n",
    "      bow_vector[feature_index] += 1\n",
    "  return bow_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-b2ec17023c2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Define training_vectors:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtraining_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_docs\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m#Just do this on the training data, which is equivalent to the code above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#print(training_vectors)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Define test_vectors:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_docs' is not defined"
     ]
    }
   ],
   "source": [
    "# Define bow_vectorizer:\n",
    "bow_vectorizer = CountVectorizer()\n",
    "#use preprocess_text \n",
    "\n",
    "# Define training_vectors:\n",
    "training_vectors = bow_vectorizer.fit_transform(training_docs)     #Just do this on the training data, which is equivalent to the code above\n",
    "#print(training_vectors)\n",
    "# Define test_vectors:\n",
    "test_vectors = bow_vectorizer.transform(test_docs)                 # on the training code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer #tfidf \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier = np.array([87,90])\n",
    "victor = np.array([93,98])\n",
    "plt.arrow(0,0,xavier[0],xavier[1], color = \"red\")\n",
    "plt.arrow(0,0, victor[0], victor[1], color = \"blue\")\n",
    "plt.axis([0,100,0,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_vec = nlp(\"happy\").vector\n",
    "sad_vec = nlp('sad').vector\n",
    "angry_vec = nlp(\"angry\").vector\n",
    "print(len(happy_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distance.cityblock(happy_vec, sad_vec))\n",
    "print(distance.euclidean(happy_vec, sad_vec))\n",
    "print(distance.cosine(happy_vec, sad_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(filtered, window=5, min_count=1, workers=2, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
